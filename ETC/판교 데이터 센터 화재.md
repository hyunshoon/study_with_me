
[if kakao 재발방지를 위한 기술적 개선](https://www.youtube.com/watch?v=9OaCT09fZ8s) 을 토대로 정리했습니다.

![](https://velog.velcdn.com/images/hyunshoon/post/6abdad45-11df-4a9b-8643-5450e92a142f/image.png)

### 서비스 복구가 늦어진 주요 원인: 시스템 전체 레이어 관점에서 철저한 이중화 부족


![](https://velog.velcdn.com/images/hyunshoon/post/bfeee09a-5e13-49bb-b2a2-703cf88fa455/image.png)

카카오는 카카오톡을 비롯한 서비스 애플리케이션들을 판교 데이터 센터 외 세 곳의 데이터센터에 다양한 형태로 분산배치해왔다. 하지만 부족했다.

판교 데이터 센터 화재로 전원공급이 중단되며 네트워크 구성 상태와 무관하게 서버가 함께 다운 -> 모니터링 및 분석 툴이 마비 ->  장비 모니터링과 장애 탐지가 원할하지 않았다.

서버 이동 및 재설치에 필요한 환경 구성정보가 대부분 판교 DC에 있어서 해당 정보 조회가 원할하지 않았다.


![](https://velog.velcdn.com/images/hyunshoon/post/2e15dd93-ecc3-4b92-8f1b-29bceb814c88/image.png)

### 앞으로 데이터 센터 전체 장애 대응을 위해 할 것

1. 모니터링 시스템 다중화
2. 메인 백본 센터 확장: 데이터 센터 간 늘어난 트래픽 대응을 위해 메인 백본 센터 2-> 3곳 확대
3. 별도 전용망 구성

![](https://velog.velcdn.com/images/hyunshoon/post/ebf5043d-f675-4546-bb42-befbd51b3981/image.png)

RDBMS, NoSQL은 데이터 센터 세 곳에 걸쳐 다중화 되어있어서 손실 없이 복구 가능

![](https://velog.velcdn.com/images/hyunshoon/post/c302030e-e09d-4dee-b585-0e404a419017/image.png)

운영관리도구는 사내 엔지니어들이 사용하는 도구로, 배포 및 소스 관리, 계정 인증등이 있다.

하지만 운영 관리도구가 일부 이중화 되어있지 않아서 장애발생시 복구가 늦었다. 특히 앱 배포도구의 경우 필수 시스템인데 가용성 인식이 부족했다.

![](https://velog.velcdn.com/images/hyunshoon/post/8dc839c2-fe96-41a0-9ff3-933ce6503d91/image.png)

카카오는 자체 클라우드와 ES 플랫폼, 레디스 카프카 등 플랫폼 도구를 클러스터 형태로 운영하고 있다.

플랫폼 도구 클러스터는 이중화가 되어 있지 않은 부분들이 있었고 데이터 센터 전면장애 대비가 없었다. 또한, 서비스 기동 우선순위 판단도 되지 않았다.

**앞으로** 모든 클러스터를 데이터 센터 단위에서 삼중화 해서 데이터센터 전면적인 장애에도 서비스 수준을 운영 하겠다. 

### 카카오 클라우드

컨테이너 오케스트레이터, 컨테이너 이미지 저장소는 이중화 되어있었다.
![](https://velog.velcdn.com/images/hyunshoon/post/d46c164f-b466-49c9-a922-5254c1b4f3f6/image.png)

하지만, 주요 메타정보 저장소, 보안 키 저장소, 오브젝트 스토리지, 클러스터 모니터링 도구 등이 단일 스토어로 구성되어 있었고 데이터센터 간 복제가 되어 있지 않았다.

데이터 유실은 되지 않았지만 메타정보 시스템 장애로 한동안 데이터 위치를 찾을 수 없었음.

스토리지 시스템의 데이터 센터 삼중화 계획중. 삼중화에 따라 발생할 수 있는 레이턴시 등 부수적인 문제는 해결해 나가겠다.

### 서비스

DB나 캐시서버는 HA 구성이 되어있었다. 

![](https://velog.velcdn.com/images/hyunshoon/post/1fce8d3e-0395-4aa0-a04c-c915eead3043/image.png)

앱은 데이터 센터 이중화가 되어 있는 컨테이너 오케스트레이터 위에서 기동되어 각 노드별로 헬스체크에서 이상 있는 노드를 자동을 제어하는 방식으로 동작하고 있었다.

하지만 자동 전환 로직이 정상 동작하지 않았고 노드의 헬스체크가 실패하면서 모든 앱이 비활성화 되며 서비스 오류가 발생.

운영관리 도구가 대부분 동작을 하지 않는 상황이어서 원인 파악의 많은 시간이 소요. 타 지역 DC의 캐시서버 활성화 이후에야 서비스 작동 가능.

## 정리

![](https://velog.velcdn.com/images/hyunshoon/post/158e2f6e-893d-489c-8312-373e8814306b/image.png)


1. 바로 서버를 기동시킬 수 있었지만 의존성 있는 다른 서버가 응답이 없을 경우 기동 불가능한 서버가 있었다.

2. 서비스 이중화가 되어있고, 페일 오버 자동화를 해놓았지만 동작 이상을 감지하는 서버가 문제가 생기자 페일오버가 동작하지 않았다.

3. 한쪽 데이터 센터가 동작 불능이 되면서 다른 데이터 센터로 트래픽이 몰리자 부하가 커지면서 파드의 응답 속도가 느려지고 헬스 체크에 실패하는 경우도 있었다.

4. 이렇게 헬스 체크에 실패한 파드들은 클라우드 오케스트레이션 서비스가 자동으로 중지시키기 때문에 전체 서비스가 내려가는 상황이 발생했고 다시 기동시키려고 했을 때에는 컨테이너 이미지 저장소의 장애로 인해 다시 실행하지 못했다.

## 해결책

![](https://velog.velcdn.com/images/hyunshoon/post/b21723e7-3033-41d1-901d-4bec977b4d84/image.png)

1. 서비스 간 의존성을 최소화 하고 중요 서비스 기능을 단독 실행 가능한 구조로 변경
2. 페일오버 구성 문제점 개선
3. 장애 대응 시나리오 재검토
4. 서버 구성정보, 배포설정 이중화

카카오는 인프라 하드웨어 설비에서 서비스 애플리케이션에 이르는 전체 시스템 레이어에서 다중화를 설계하고 구축해 가겠다.

## 할 것

#### 전체 레이어 다중화
#### 서비스 간 우선순위 체계화
#### 장애 대비 훈련 확대 실시
#### 자체 구축 데이터센터 디자인 개선


























